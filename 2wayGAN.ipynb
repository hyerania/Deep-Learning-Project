{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from scipy.misc import toimage\n",
    "import skimage.color\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device('cuda')     # Default CUDA device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 512\n",
    "BETA1 = 0.5\n",
    "BETA2 = 0.999\n",
    "LAMBDA = 10\n",
    "ALPHA = 1000\n",
    "BATCH_SIZE = 5\n",
    "NUM_EPOCHS = 5\n",
    "LATENT_DIM = 100\n",
    "TRAIN_NUM = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator 1 Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    direction = 1;\n",
    "    \n",
    "    def changeDirection(self,i):\n",
    "        self.direction = i\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        #  Convolutional layers \n",
    "        \n",
    "        # input 512x512x3  output 512x512x16\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5, stride = 1, padding = 2)\n",
    "        self.conv1_bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv1_bn2 = nn.BatchNorm2d(16)\n",
    "        # input 512x512x16  output 256x256x32\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5, stride = 2, padding = 2)\n",
    "        self.conv2_bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2_bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # input 265x256x32  output 128x128x64\n",
    "        self.conv3 = nn.Conv2d(32, 64, 5, stride = 2, padding = 2)\n",
    "        self.conv3_bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv3_bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # input 128x128x64  output 64x64x128\n",
    "        self.conv4 = nn.Conv2d(64, 128, 5, stride = 2, padding = 2)\n",
    "        self.conv4_bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv4_bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # input 64x64x128  output 32x32x128\n",
    "        # the output of this layer we need layers for global features\n",
    "        self.conv5 = nn.Conv2d(128, 128, 5, stride = 2, padding = 2)\n",
    "        self.conv5_bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv5_bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # convs for global features\n",
    "        # input 32x32x128 output 16x16x128\n",
    "        self.conv51 = nn.Conv2d(128,128,5, stride =2 , padding =2 )\n",
    "        \n",
    "        # input 16x16x128 output 8x8x128\n",
    "        self.conv52 = nn.Conv2d(128,128,5, stride =2 , padding =2 )\n",
    "        \n",
    "        # input 8x8x128 output 1x1x128\n",
    "        self.conv531 = nn.Conv2d(128,128,5, stride =2 , padding =1 )\n",
    "        \n",
    "        # input 1x1x128 output 1x1x128\n",
    "        self.conv532 = nn.Conv2d(128,128,5, stride =2 , padding =1 )\n",
    "        \n",
    "        # input 32x32x128 output 32x32x128\n",
    "        # the global features should be concatenated to the feature map aftere this layer\n",
    "        # the output after concat would be 32x32x256\n",
    "        self.conv6 = nn.Conv2d(128, 128, 5, stride = 1, padding =2)\n",
    "        \n",
    "        # input 32x32x256 output 32x32x128\n",
    "        self.conv7 = nn.Conv2d(256, 128, 5, stride = 1, padding = 2)\n",
    "        \n",
    "        # deconvolutional layers\n",
    "        # input 32x32x128 output 64x64x128\n",
    "        self.dconv1 = nn.ConvTranspose2d(128, 128, 4, stride = 2, padding = 1)\n",
    "        self.dconv1_bn1 = nn.BatchNorm2d(128)\n",
    "        self.dconv1_bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # input 64x64x256 ouput 128x128x128\n",
    "        self.dconv2 = nn.ConvTranspose2d(256, 128, 4, stride = 2, padding = 1)\n",
    "        self.dconv2_bn1 = nn.BatchNorm2d(256)\n",
    "        self.dconv2_bn2 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # input 128x128x192 output 256x256x64\n",
    "        self.dconv3 = nn.ConvTranspose2d(192, 64, 4, stride = 2, padding = 1)\n",
    "        self.dconv3_bn1 = nn.BatchNorm2d(192)\n",
    "        self.dconv3_bn2 = nn.BatchNorm2d(192)\n",
    "        \n",
    "        # input 256x256x96 ouput 512x512x32\n",
    "        self.dconv4 = nn.ConvTranspose2d(96, 32, 4, stride = 2, padding = 1)\n",
    "        self.dconv4_bn1 = nn.BatchNorm2d(96)\n",
    "        self.dconv4_bn2 = nn.BatchNorm2d(96)\n",
    "        \n",
    "        # final convolutional layers\n",
    "        # input 512x512x48 output 512x512x16\n",
    "        self.conv8 = nn.Conv2d(48, 16, 5, stride = 1, padding = 2)\n",
    "        self.conv8_bn1 = nn.BatchNorm2d(48)\n",
    "        self.conv8_bn2 = nn.BatchNorm2d(48)\n",
    "        \n",
    "        # input 512x512x16 output 512x512x3\n",
    "        self.conv9 = nn.Conv2d(16, 3, 5, stride = 1, padding = 2)    \n",
    "        self.conv9_bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv9_bn2 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        # SELU\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if(self.direction == 1):\n",
    "            self.forward_step_dir_1(x)\n",
    "        else:\n",
    "            self.forward_step_dir_2(x)\n",
    "\n",
    "            \n",
    "    def forward_step_dir_1(self, x):\n",
    "            # input 512x512x3 to output 512x512x16\n",
    "        x = self.conv1_bn1(F.selu(self.conv1(x)))\n",
    "        \n",
    "#         print(\"x\")\n",
    "#         print(x.shape)\n",
    "        # input 512x512x16 to output 256x256x32\n",
    "        x1 = self.conv2_bn1(F.selu(self.conv2(x)))\n",
    "#         print(\"x1\")\n",
    "#         print(x1.shape)\n",
    "        # input 256x256x32 to output 128x128x64\n",
    "        x2 = self.conv3_bn1(F.selu(self.conv3(x1)))\n",
    "#         print(\"x2\")\n",
    "#         print(x2.shape)\n",
    "        # input 128x128x64 to output 64x64x128\n",
    "        x3 = self.conv4_bn1(F.selu(self.conv4(x2)))\n",
    "#         print(\"x3\")\n",
    "#         print(x3.shape)\n",
    "        # input 64x64x128 to output 32x32x128\n",
    "        x4 = self.conv5_bn1(F.selu(self.conv5(x3)))\n",
    "#         print(\"x4\")\n",
    "#         print(x4.shape)\n",
    "        #convolutions for global features\n",
    "        # input 32x32x128 to output 16x16x128\n",
    "        x51 = self.conv51(x4)\n",
    "#         print(\"x51\")\n",
    "#         print(x51.shape)\n",
    "        # input 16x16x128 to output 8x8x128\n",
    "        x52 = self.conv52(x51)\n",
    "#         print(\"x52\")\n",
    "#         print(x52.shape)\n",
    "        # input 8x8x128 to output 1x1x128\n",
    "        x53 = self.conv532(F.selu(self.conv531(x52)))\n",
    "#         print(\"x53\")\n",
    "#         print(x53.shape)\n",
    "        x53_temp = torch.cat([x53]*32,dim = 2 )\n",
    "        x53_temp = torch.cat([x53_temp]*32,dim=3)\n",
    "#         print(\"x53_temp\")\n",
    "#         print(x53_temp.shape)\n",
    "        \n",
    "        # input 32x32x256 to output 32x32x128\n",
    "        x5 = self.conv6(x4)\n",
    "#         print(\"x5\")\n",
    "#         print(x5.shape)\n",
    "        # input 32x32x128 to output 32x32x128\n",
    "        x5 = self.conv7(torch.cat([x5,x53_temp],dim=1))\n",
    "#         print(\"x5\")\n",
    "#         print(x5.shape)\n",
    "        # input 32x32x128 to output 64x64x128\n",
    "        xd = self.dconv1(self.dconv1_bn1(F.selu(x5)))\n",
    "#         print(\"xd1\")\n",
    "#         print(xd.shape)\n",
    "        # input 64x64x256 to output 128x128x128\n",
    "        xd = self.dconv2(self.dconv2_bn1(F.selu(torch.cat([xd,x3], dim=1))))\n",
    "#         print(\"xd2\")\n",
    "#         print(xd.shape)\n",
    "        # input 128x128x192 to output 256x256x64\n",
    "        xd = self.dconv3(self.dconv3_bn1(F.selu(torch.cat([xd,x2],dim=1))))\n",
    "#         print(\"xd3\")\n",
    "#         print(xd.shape)\n",
    "        # input 256x256x64 to output 512x512x32\n",
    "        xd = self.dconv4(self.dconv4_bn1(F.selu(torch.cat([xd,x1],dim=1))))\n",
    "#         print(\"xd4\")\n",
    "#         print(xd.shape)\n",
    "        # input 512x512x48 to output 512x512x16\n",
    "        xd = self.conv8(self.conv8_bn1(F.selu(torch.cat([xd,x],dim=1))))\n",
    "#         print(\"xd 8\")\n",
    "#         print(xd.shape)\n",
    "        # input 512x512x16 to output 512x512x3\n",
    "        xd = self.conv9(self.conv9_bn1(F.selu((xd))))\n",
    "#         print(\"xd 9\")\n",
    "#         print(xd.shape)\n",
    "        return xd\n",
    "\n",
    "\n",
    "    def forward_step_dir_2(self, x):\n",
    "    # input 512x512x3 to output 512x512x16\n",
    "        x = self.conv1_bn2(F.selu(self.conv1(x)))\n",
    "        \n",
    "#         print(\"x\")\n",
    "#         print(x.shape)\n",
    "        # input 512x512x16 to output 256x256x32\n",
    "        x1 = self.conv2_bn2(F.selu(self.conv2(x)))\n",
    "#         print(\"x1\")\n",
    "#         print(x1.shape)\n",
    "        # input 256x256x32 to output 128x128x64\n",
    "        x2 = self.conv3_bn2(F.selu(self.conv3(x1)))\n",
    "#         print(\"x2\")\n",
    "#         print(x2.shape)\n",
    "        # input 128x128x64 to output 64x64x128\n",
    "        x3 = self.conv4_bn2(F.selu(self.conv4(x2)))\n",
    "#         print(\"x3\")\n",
    "#         print(x3.shape)\n",
    "        # input 64x64x128 to output 32x32x128\n",
    "        x4 = self.conv5_bn2(F.selu(self.conv5(x3)))\n",
    "#         print(\"x4\")\n",
    "#         print(x4.shape)\n",
    "        #convolutions for global features\n",
    "        # input 32x32x128 to output 16x16x128\n",
    "        x51 = self.conv51(x4)\n",
    "#         print(\"x51\")\n",
    "#         print(x51.shape)\n",
    "        # input 16x16x128 to output 8x8x128\n",
    "        x52 = self.conv52(x51)\n",
    "#         print(\"x52\")\n",
    "#         print(x52.shape)\n",
    "        # input 8x8x128 to output 1x1x128\n",
    "        x53 = self.conv532(F.selu(self.conv531(x52)))\n",
    "#         print(\"x53\")\n",
    "#         print(x53.shape)\n",
    "        x53_temp = torch.cat([x53]*32,dim = 2 )\n",
    "        x53_temp = torch.cat([x53_temp]*32,dim=3)\n",
    "#         print(\"x53_temp\")\n",
    "#         print(x53_temp.shape)\n",
    "        \n",
    "        # input 32x32x256 to output 32x32x128\n",
    "        x5 = self.conv6(x4)\n",
    "#         print(\"x5\")\n",
    "#         print(x5.shape)\n",
    "        # input 32x32x128 to output 32x32x128\n",
    "        x5 = self.conv7(torch.cat([x5,x53_temp],dim=1))\n",
    "#         print(\"x5\")\n",
    "#         print(x5.shape)\n",
    "        # input 32x32x128 to output 64x64x128\n",
    "        xd = self.dconv1(self.dconv1_bn2(F.selu(x5)))\n",
    "#         print(\"xd1\")\n",
    "#         print(xd.shape)\n",
    "        # input 64x64x256 to output 128x128x128\n",
    "        xd = self.dconv2(self.dconv2_bn2(F.selu(torch.cat([xd,x3], dim=1))))\n",
    "#         print(\"xd2\")\n",
    "#         print(xd.shape)\n",
    "        # input 128x128x192 to output 256x256x64\n",
    "        xd = self.dconv3(self.dconv3_bn2(F.selu(torch.cat([xd,x2],dim=1))))\n",
    "#         print(\"xd3\")\n",
    "#         print(xd.shape)\n",
    "        # input 256x256x64 to output 512x512x32\n",
    "        xd = self.dconv4(self.dconv4_bn2(F.selu(torch.cat([xd,x1],dim=1))))\n",
    "#         print(\"xd4\")\n",
    "#         print(xd.shape)\n",
    "        # input 512x512x48 to output 512x512x16\n",
    "        xd = self.conv8(self.conv8_bn2(F.selu(torch.cat([xd,x],dim=1))))\n",
    "#         print(\"xd 8\")\n",
    "#         print(xd.shape)\n",
    "        # input 512x512x16 to output 512x512x3\n",
    "        xd = self.conv9(self.conv9_bn2(F.selu((xd))))\n",
    "#         print(\"xd 9\")\n",
    "#         print(xd.shape)\n",
    "        return xd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        #  Convolutional layers \n",
    "        \n",
    "        # input 512x512x3  output 512x512x16\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5, stride = 1, padding = 2)\n",
    "        self.conv1_in = nn.InstanceNorm2d(16)\n",
    "        \n",
    "        # input 512x512x16  output 256x256x32\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5, stride = 2, padding = 2)\n",
    "        self.conv2_in = nn.InstanceNorm2d(32)\n",
    "        \n",
    "        # input 265x256x32  output 128x128x64\n",
    "        self.conv3 = nn.Conv2d(32, 64, 5, stride = 2, padding = 2)\n",
    "        self.conv3_in = nn.InstanceNorm2d(64)\n",
    "        \n",
    "        # input 128x128x64  output 64x64x128\n",
    "        self.conv4 = nn.Conv2d(64, 128, 5, stride = 2, padding = 2)\n",
    "        self.conv4_in = nn.InstanceNorm2d(128)\n",
    "        \n",
    "        # input 64x64x128  output 32x32x128\n",
    "        # the output of this layer we need layers for global features\n",
    "        self.conv5 = nn.Conv2d(128, 128, 5, stride = 2, padding = 2)\n",
    "        self.conv5_in = nn.InstanceNorm2d(128)\n",
    "        \n",
    "        # input 32x32x128  output 16x16x128\n",
    "        # the output of this layer we need layers for global features\n",
    "        self.conv6 = nn.Conv2d(128, 128, 5, stride = 2, padding = 2)\n",
    "        self.conv6_in = nn.InstanceNorm2d(128)\n",
    "        \n",
    "        # input 16x16x128  output 1x1x1\n",
    "        # the output of this layer we need layers for global features\n",
    "        self.conv7 = nn.Conv2d(128, 1, 16)\n",
    "        self.conv7_in = nn.InstanceNorm2d(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # input 512x512x3 to output 512x512x16\n",
    "        x = self.conv1_in(F.leaky_relu(self.conv1(x)))\n",
    "#         print(\"x1\")\n",
    "#         print(x.shape)\n",
    "        # input 512x512x16 to output 256x256x32\n",
    "        x = self.conv2_in(F.leaky_relu(self.conv2(x)))\n",
    "#         print(\"x2\")\n",
    "#         print(x.shape)\n",
    "        \n",
    "        # input 256x256x32 to output 128x128x64\n",
    "        x = self.conv3_in(F.leaky_relu(self.conv3(x)))\n",
    "#         print(\"x3\")\n",
    "#         print(x.shape)\n",
    "        \n",
    "        # input 128x128x64 to output 64x64x128\n",
    "        x = self.conv4_in(F.leaky_relu(self.conv4(x)))\n",
    "#         print(\"x4\")\n",
    "#         print(x.shape)\n",
    "        \n",
    "        # input 64x64x128 to output 32x32x128\n",
    "        x = self.conv5_in(F.leaky_relu(self.conv5(x)))\n",
    "#         print(\"x5\")\n",
    "#         print(x.shape)\n",
    "        \n",
    "        # input 32x32x128 to output 16x16x128\n",
    "        x = self.conv6_in(F.leaky_relu(self.conv6(x)))\n",
    "#         print(\"x6\")\n",
    "#         print(x.shape)\n",
    "        \n",
    "        # input 16x16x128 to output 1x1x1\n",
    "        x = self.conv7(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = Generator()\n",
    "generator1.changeDirection(1)\n",
    "generator2 = Generator()\n",
    "generator2.changeDirection(1)\n",
    "discriminator = Discriminator()\n",
    "# print(generator1)\n",
    "# print(generator2)\n",
    "# print(discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Training and Test Set Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the images for PILImage to tensor, so they can be accepted as the input to the network\n",
    "transform = transforms.Compose([transforms.Resize((SIZE,SIZE), interpolation=2),transforms.ToTensor()])\n",
    "\n",
    "trainset_1_gt =torchvision.datasets.ImageFolder(root='./images/Expert-C/Training1/', transform=transform, target_transform=None)    \n",
    "trainset_2_gt =torchvision.datasets.ImageFolder(root='./images/Expert-C/Training2/', transform=transform, target_transform=None)    \n",
    "testset_gt =torchvision.datasets.ImageFolder(root='./images/Expert-C/Testing/', transform=transform, target_transform=None)    \n",
    "\n",
    "# Converting the images for PILImage to tensor, so they can be accepted as the input to the network\n",
    "trainset_1_inp =torchvision.datasets.ImageFolder(root='./images/tif-images/Training1/', transform=transform, target_transform=None)    \n",
    "trainset_2_inp =torchvision.datasets.ImageFolder(root='./images/tif-images/Training2/', transform=transform, target_transform=None)    \n",
    "testset_inp =torchvision.datasets.ImageFolder(root='./images/tif-images/Testing/', transform=transform, target_transform=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return tuple(d[i] for d in self.datasets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(d) for d in self.datasets)\n",
    "\n",
    "\n",
    "trainLoader1 = torch.utils.data.DataLoader(\n",
    "             ConcatDataset(\n",
    "                 trainset_1_gt,\n",
    "                 trainset_1_inp\n",
    "             ),\n",
    "             batch_size=BATCH_SIZE, shuffle=True,)\n",
    "\n",
    "trainLoader2 = torch.utils.data.DataLoader(\n",
    "             ConcatDataset(\n",
    "                 trainset_2_gt,\n",
    "                 trainset_2_inp\n",
    "             ),\n",
    "             batch_size=BATCH_SIZE, shuffle=True,)\n",
    "\n",
    "testLoader = torch.utils.data.DataLoader(\n",
    "             ConcatDataset(\n",
    "                 testset_gt,\n",
    "                 testset_inp\n",
    "             ),\n",
    "             batch_size=BATCH_SIZE, shuffle=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x0000016AA6854080>\n",
      "<torch.utils.data.dataloader._DataLoaderIter object at 0x0000016AA34FD588>\n",
      "torch.Size([5, 3, 512, 512])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 3, 512, 512])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "print(trainLoader1)\n",
    "\n",
    "dataiter = iter(trainLoader1)\n",
    "print(dataiter)\n",
    "(target,input) = dataiter.next()\n",
    "print(target[0].shape)\n",
    "print(target[1].shape)\n",
    "print(input[0].shape)\n",
    "print(input[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer_g1 = optim.Adam(generator1.parameters(), lr = 0.001, betas=(BETA1,BETA2))\n",
    "optimizer_g2 = optim.Adam(generator2.parameters(), lr = 0.001, betas=(BETA1,BETA2))\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr = 0.001, betas=(BETA1,BETA2))\n",
    "\n",
    "# Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "Tensor = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Penalty\n",
    "Computes gradient penalty loss for A-WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeGradientPenalty(D, realSample, fakeSample):\n",
    "    alpha = Tensor(np.random.random((realSample.shape)))\n",
    "    interpolates = (alpha * realSample + ((1 - alpha) * fakeSample)).requires_grad_(True)\n",
    "    dInterpolation = D(interpolates)\n",
    "    fakeOutput = Variable(Tensor(realSample.shape[0],1,1,1).fill_(1.0), requires_grad=False)\n",
    "    \n",
    "    gradients = autograd.grad(\n",
    "        outputs = dInterpolation,\n",
    "        inputs = interpolates,\n",
    "        grad_outputs = fakeOutput,\n",
    "        create_graph = True,\n",
    "        retain_graph = True,\n",
    "        only_inputs = True)[0]\n",
    "    \n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    maxVals = []\n",
    "    normGradients = gradients.norm(2, dim=1)-1\n",
    "    for i in range(len(normGradients)):\n",
    "        if(normGradients[i] > 0):\n",
    "            maxVals.append(normGradients[i].detach().numpy())\n",
    "        else:\n",
    "            maxVals.append(0)\n",
    "\n",
    "    gradientPenalty = np.mean(maxVals)\n",
    "    return gradientPenalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Loss 1 WAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatorAdversarialLoss( output_images):\n",
    "    validity = discriminator(output_images)\n",
    "    gen_adv_loss = torch.mean(validity)\n",
    "    \n",
    "    return gen_adv_loss\n",
    "\n",
    "# def computeGeneratorLoss(inputs, outputs_g1, outputs_g2):\n",
    "def computeGeneratorLoss(inputs, outputs_g1):\n",
    "    # generator 1\n",
    "    gen_adv_loss1 = generatorAdversarialLoss(outputs_g1)\n",
    "    \n",
    "#     generator 2\n",
    "#     gen_adv_loss2 = generatorAdversarialLoss(outputs_g2)\n",
    "    \n",
    "    i_loss = criterion(inputs, outputs_g1)\n",
    "    gen_loss = -gen_adv_loss1 + ALPHA*i_loss\n",
    "    \n",
    "    return gen_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Loss 2 WAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#losses of the GAN\n",
    "def generatorAdversarialLoss(output_images):\n",
    "    validity = discriminator(output_images)\n",
    "    gen_adv_loss = torch.mean(validity)\n",
    "    \n",
    "    return gen_adv_loss\n",
    "\n",
    "# loss between the input and the input that is generated after passing the input to both the generators\n",
    "def computeConsistencLoss(input, generated_input):\n",
    "    return criterion(input, generated_input)\n",
    "\n",
    "# loss between the input and the output generated from one generator\n",
    "def computeIdentityLoss(input, generated_output):\n",
    "    return criterion(input, generated_output)\n",
    "\n",
    "# includes the adversarial loss and identity mapping loss for one generator\n",
    "# can use it on both the generators\n",
    "\n",
    "def computeGeneratorLoss(inputs,outputs):\n",
    "    # generator 1\n",
    "    gen_adv_loss1 = generatorAdversarialLoss(outputs_g1)\n",
    "    \n",
    "     # generator1 Identity mapping loss\n",
    "    i_loss_1 = criterion(inputs_g1, outputs_g1)\n",
    "    \n",
    "    gen_loss = -gen_adv_loss1 + ALPHA*i_loss\n",
    "    \n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminatorLoss(d1Real, d1Fake, gradPenalty):\n",
    "    return (-torch.mean(d1Fake) + torch.mean(d1Real)) - (LAMBDA*gradPenalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mse_loss(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-411fcf9f2c4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mgenerated_enhanced_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munenhanced_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#         generated_unenhanced_image = generator2(generated_enhanced_image)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mloss_g1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerated_enhanced_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menhanced_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mloss_g1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0moptimizer_g1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\dellatsni\\adocana\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\dellatsni\\adocana\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\dellatsni\\adocana\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   1714\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1715\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1716\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pointwise_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\dellatsni\\adocana\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_pointwise_loss\u001b[1;34m(lambd, lambd_optimized, input, target, reduction)\u001b[0m\n\u001b[0;32m   1672\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'elementwise_mean'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1673\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1674\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlambd_optimized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: mse_loss(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "# trained on the first 2250 images\n",
    "\n",
    "batches_done = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i,  (target, input) in enumerate(trainLoader1, 0):\n",
    "#         print(i)\n",
    "#         print(target[0].shape)\n",
    "#         print(input[0].shape)\n",
    "        unenhanced_image = input[0]\n",
    "        enhanced_image = target[0] \n",
    "        \n",
    "        optimizer_g1.zero_grad()\n",
    "        print(unenhanced_image)\n",
    "        generated_enhanced_image = generator1(unenhanced_image)\n",
    "        print(generated_enhanced_image)\n",
    "        generated_unenhanced_image = generator2(generated_enhanced_image)\n",
    "        print(generated_unenhanced_image)\n",
    "        loss_g1 = np.log(criterion(generated_enhanced_image, enhanced_image))\n",
    "        loss_g1.backward()\n",
    "        optimizer_g1.step()\n",
    "        loss_g2 = np.log(criterion(generated_unenhanced_image, unenhanced_image))\n",
    "        loss_g1.backward()\n",
    "        optimizer_g2.step()\n",
    "        \n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 225 == 224:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.5f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 225))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batches_done = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i,  (target, input) in enumerate(trainLoader2, 0):\n",
    "        unenhanced, dummy = input\n",
    "        groundTruth, dummy = target\n",
    "    \n",
    "        g1_inputs = Variable(unenhanced.type(Tensor))\n",
    "        realImgs = Variable(groundTruth.type(Tensor))\n",
    "        \n",
    "        ### GENERATOR 1\n",
    "        optimizer_g1.zero_grad()\n",
    "        generated_enhanced = generator1(g1_inputs)\n",
    "        \n",
    "        # Adversarial loss GAN 1 \n",
    "        gen_validity = discriminator(generated_enhanced)\n",
    "        \n",
    "        ### DISCRIMINATOR\n",
    "        optimizer_d.zero_grad()\n",
    "        \n",
    "        # Real Images\n",
    "        realValid = discriminator(realImgs)\n",
    "        # Fake Images\n",
    "        fakeValid = discriminator(generated_enhanced)\n",
    "        \n",
    "        gradientPenalty = computeGradientPenalty(discriminator, realImgs.data, fakeImgs.data)\n",
    "        dLoss = discriminatorLoss(realValid, fakeValid, gradientPenalty)\n",
    "        dLoss.backward()\n",
    "        optimizer_d.step()\n",
    "        optimizer_g1.zero_grad()\n",
    "        \n",
    "        ### TRAIN GENERATOR\n",
    "        if TRAIN_NUM % 50 == 0:\n",
    "            print(\"Training Generator on Iteration: %d\" % (i))\n",
    "            # Generate a batch of images\n",
    "            fake_imgs = generator1(noiseImgs)\n",
    "            residual_learning = fake_imgs + noiseImgs\n",
    "            gLoss = computeGeneratorLoss(noiseImgs,residual_learning)\n",
    "\n",
    "            gLoss.backward()\n",
    "            optimizer_g1.step()\n",
    "    \n",
    "            print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: |%f] [G loss: %f]\" % (epoch, NUM_EPOCHS , i, len(trainloader), dLoss.item(), gLoss.item()))\n",
    "            f = open(\"logStatus.txt\",\"a+\")\n",
    "            f.write(\"[Epoch %d/%d] [Batch %d/%d] [D loss: |%f] [G loss: %f]\\n\" % (epoch, NUM_EPOCHS , i, len(trainloader), dLoss.item(), gLoss.item()))\n",
    "            f.close()\n",
    "            \n",
    "            if batches_done % 200 == 0:\n",
    "                save_image(fake_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "\n",
    "            batches_done += TRAIN_NUM\n",
    "            print(\"Done training generator on iteration: %d\" % (i))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
